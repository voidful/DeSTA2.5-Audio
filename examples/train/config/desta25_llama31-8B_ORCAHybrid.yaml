hydra:
  run:
    dir: ${exp_dir}


project: a100
name: desta25_orca
exp_dir: ???

resume_from_checkpoint: null
init_from_pretrained_weights: null

ptl_module: default

wandb:
  project: desta25
  log_model: all

trainer:
  devices: 8 # 0 for CPU, or list of the GPUs to use e.g. [0, 1] or [0]
  num_nodes: 1
  max_epochs: 5
  max_steps: -1 # precedence over max_epochs
  accumulate_grad_batches: 1 # accumulates grads every k batches
  gradient_clip_val: 1.0
  precision: "bf16-mixed" #16 # should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  log_every_n_steps: 10  # interval of logging.
  val_check_interval: 0.5  # set to 0.25 to check 4 times per epoch, or an int for number of iterations
  num_sanity_val_steps: 0 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: True


model:
  llm:
    model_id: DeSTA-ntu/Llama-3.1-8B-Instruct
    freeze: True

  encoder:
    model_id: openai/whisper-large-v3
    freeze: True

  connector:
    mode: orca_hybrid
    prompt_size: 64  # Not used in ORCA mode, but kept for compatibility
    num_hidden_layers: 6

  # ORCA-DeSTA specific configuration
  orca:
    enabled: true
    local_enabled: true  # Set to false to disable local downsample features
    global_cross_attn: true  # Global tokens also use cross-attention instead of concat
    deep_injection_enabled: true  # Gated cross-attention in LLM layers
    global_num_tokens: 8
    local_downsample: 2
    local_kernel_size: 5
    gate_init: 0.1
    ortho_weight_global: 0.05
    ortho_diversity_weight: 0.05
    ortho_weight_qformer_local: 0.05  # Orthogonality between global and local tokens
    align_weight_local: 0.05  # Alignment loss to bring local tokens closer to text

  placeholder_token: "<|reserved_special_token_87|>" # need to be in LLM embedding table, but will be replaced by audio embedding
  audio_locator: "<|AUDIO|>" # newly created token

  generation_kwargs:
    max_new_tokens: 192
    do_sample: False
    temperature: 1
    top_p: 1

optim:
  lr: 1e-4
  betas: [0.9, 0.98]
  weight_decay: 0.01

  sched:
    warmup_steps: 5000
